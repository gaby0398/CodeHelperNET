import os
import re
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer, CrossEncoder
from chromadb import PersistentClient
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import numpy as np

class RAGChatbot:
    def __init__(self, db_path: str = "./vector_db"):
        """Inicializar el chatbot RAG completo"""
        self.db_path = db_path
        self.client = PersistentClient(path=db_path)
        
        # Conectar a la colecciÃ³n mejorada
        self.collection = self.client.get_collection("codehelper_csharp_improved")
        
        # Modelo de embeddings para recuperaciÃ³n
        self.embedding_model = SentenceTransformer("microsoft/codebert-base-mlm")
        
        # Cross-encoder para re-ranking (mejora la calidad de resultados)
        self.cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
        
        # Modelo LLM para generaciÃ³n (usando un modelo mÃ¡s pequeÃ±o pero efectivo)
        self.setup_llm()
        
        # Traductor para respuestas
        self.translator = pipeline("translation_en_to_es", model="Helsinki-NLP/opus-mt-en-es")
        
        # Templates de prompts
        self.setup_prompts()
        
    def setup_llm(self):
        """Configurar modelo LLM para generaciÃ³n"""
        try:
            # Usar un modelo mÃ¡s pequeÃ±o pero efectivo para generaciÃ³n
            model_name = "microsoft/DialoGPT-medium"
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto" if torch.cuda.is_available() else "cpu"
            )
            self.tokenizer.pad_token = self.tokenizer.eos_token
        except Exception as e:
            print(f"Error cargando modelo LLM: {e}")
            print("Usando modo de solo recuperaciÃ³n...")
            self.model = None
            self.tokenizer = None
    
    def setup_prompts(self):
        """Configurar templates de prompts para diferentes tipos de preguntas"""
        self.prompts = {
            'code_example': PromptTemplate(
                input_variables=["context", "question"],
                template="""Eres un experto en C# y .NET. BasÃ¡ndote en el siguiente contexto, responde la pregunta del usuario.

Contexto:
{context}

Pregunta: {question}

Responde de manera clara y concisa, incluyendo ejemplos de cÃ³digo cuando sea apropiado. Si la informaciÃ³n del contexto no es suficiente, indÃ­calo claramente.

Respuesta:"""
            ),
            'concept_explanation': PromptTemplate(
                input_variables=["context", "question"],
                template="""Eres un profesor experto en C# y .NET. Explica el concepto solicitado usando el siguiente contexto como referencia.

Contexto:
{context}

Concepto a explicar: {question}

Proporciona una explicaciÃ³n clara, estructurada y fÃ¡cil de entender. Incluye ejemplos prÃ¡cticos cuando sea posible.

ExplicaciÃ³n:"""
            ),
            'syntax_help': PromptTemplate(
                input_variables=["context", "question"],
                template="""Eres un asistente de programaciÃ³n especializado en C#. Ayuda con la sintaxis solicitada usando el siguiente contexto.

Contexto:
{context}

Pregunta sobre sintaxis: {question}

Proporciona la sintaxis correcta, ejemplos de uso y explicaciones claras. Incluye casos comunes y mejores prÃ¡cticas.

Respuesta:"""
            )
        }
    
    def classify_question(self, question: str) -> str:
        """Clasificar el tipo de pregunta para usar el prompt apropiado"""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ['ejemplo', 'cÃ³digo', 'implementar', 'cÃ³mo hacer']):
            return 'code_example'
        elif any(word in question_lower for word in ['quÃ© es', 'explicar', 'concepto', 'definir']):
            return 'concept_explanation'
        elif any(word in question_lower for word in ['sintaxis', 'syntax', 'formato', 'escribir']):
            return 'syntax_help'
        else:
            return 'code_example'  # Default
    
    def retrieve_relevant_chunks(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """Recuperar chunks relevantes usando embeddings"""
        # Generar embedding de la consulta
        query_embedding = self.embedding_model.encode(query).tolist()
        
        # BÃºsqueda inicial
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results * 2  # Obtener mÃ¡s resultados para re-ranking
        )
        
        # Re-ranking con cross-encoder
        if len(results['documents'][0]) > 0:
            pairs = [[query, doc] for doc in results['documents'][0]]
            scores = self.cross_encoder.predict(pairs)
            
            # Combinar documentos con scores
            doc_scores = list(zip(results['documents'][0], scores, results['metadatas'][0]))
            doc_scores.sort(key=lambda x: x[1], reverse=True)
            
            # Retornar los mejores resultados
            top_results = []
            for doc, score, metadata in doc_scores[:n_results]:
                top_results.append({
                    'content': doc,
                    'score': score,
                    'metadata': metadata
                })
            
            return top_results
        
        return []
    
    def generate_response(self, context: str, question: str, question_type: str) -> str:
        """Generar respuesta usando el modelo LLM"""
        if self.model is None:
            return "Lo siento, el modelo de generaciÃ³n no estÃ¡ disponible. AquÃ­ estÃ¡n los fragmentos mÃ¡s relevantes:\n\n" + context
        
        try:
            # Seleccionar prompt apropiado
            prompt_template = self.prompts[question_type]
            prompt = prompt_template.format(context=context, question=question)
            
            # Tokenizar
            inputs = self.tokenizer.encode(prompt, return_tensors="pt", truncation=True, max_length=1024)
            
            # Generar respuesta
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 200,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # Decodificar respuesta
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extraer solo la parte generada (despuÃ©s del prompt)
            response = response[len(prompt):].strip()
            
            # Limpiar respuesta
            response = re.sub(r'^Respuesta:\s*', '', response)
            response = re.sub(r'^ExplicaciÃ³n:\s*', '', response)
            
            return response if response else "No pude generar una respuesta especÃ­fica con la informaciÃ³n disponible."
            
        except Exception as e:
            print(f"Error en generaciÃ³n: {e}")
            return "Error generando respuesta. AquÃ­ estÃ¡n los fragmentos mÃ¡s relevantes:\n\n" + context
    
    def translate_response(self, response: str) -> str:
        """Traducir respuesta al espaÃ±ol si es necesario"""
        try:
            # Detectar si ya estÃ¡ en espaÃ±ol
            spanish_words = ['es', 'son', 'estÃ¡', 'estÃ¡n', 'para', 'con', 'por', 'que', 'como', 'cuando']
            if any(word in response.lower() for word in spanish_words):
                return response
            
            # Traducir
            translated = self.translator(response[:500])[0]["translation_text"]
            return translated
        except:
            return response
    
    def chat(self, question: str) -> str:
        """Proceso completo de chat RAG"""
        print(f"ğŸ¤– Procesando pregunta: {question}")
        
        # 1. Clasificar pregunta
        question_type = self.classify_question(question)
        print(f"ğŸ“ Tipo de pregunta: {question_type}")
        
        # 2. Recuperar contexto relevante
        relevant_chunks = self.retrieve_relevant_chunks(question, n_results=3)
        
        if not relevant_chunks:
            return "âŒ No encontrÃ© informaciÃ³n relevante para tu pregunta. Â¿PodrÃ­as reformularla?"
        
        # 3. Preparar contexto
        context_parts = []
        for i, chunk in enumerate(relevant_chunks):
            context_parts.append(f"Fragmento {i+1} (Score: {chunk['score']:.3f}):\n{chunk['content']}\n")
        
        context = "\n".join(context_parts)
        
        # 4. Generar respuesta
        print("ğŸ§  Generando respuesta...")
        response = self.generate_response(context, question, question_type)
        
        # 5. Traducir si es necesario
        response = self.translate_response(response)
        
        return response
    
    def interactive_chat(self):
        """Modo interactivo de chat"""
        print("ğŸ¤– ChatBot RAG para C# y .NET")
        print("Escribe 'salir' para terminar")
        print("-" * 50)
        
        while True:
            try:
                question = input("\nğŸ‘¤ TÃº: ").strip()
                
                if question.lower() in ['salir', 'exit', 'quit']:
                    print("ğŸ‘‹ Â¡Hasta luego!")
                    break
                
                if not question:
                    continue
                
                # Procesar pregunta
                response = self.chat(question)
                
                print(f"\nğŸ¤– Asistente: {response}")
                
                # OpciÃ³n para ver contexto
                show_context = input("\nÂ¿Ver contexto usado? (s/n): ").strip().lower()
                if show_context == 's':
                    relevant_chunks = self.retrieve_relevant_chunks(question, n_results=3)
                    print("\nğŸ“š Contexto utilizado:")
                    for i, chunk in enumerate(relevant_chunks):
                        print(f"\n--- Fragmento {i+1} ---")
                        print(f"Tipo: {chunk['metadata']['content_type']}")
                        print(f"Archivo: {chunk['metadata']['filename']}")
                        print(f"Score: {chunk['score']:.3f}")
                        print(f"Contenido: {chunk['content'][:200]}...")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Â¡Hasta luego!")
                break
            except Exception as e:
                print(f"âŒ Error: {e}")

if __name__ == "__main__":
    chatbot = RAGChatbot()
    chatbot.interactive_chat() 