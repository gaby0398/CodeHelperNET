import os
import re
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer, CrossEncoder
from chromadb import PersistentClient
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import numpy as np

class RAGChatbot:
    def __init__(self, db_path: str = "./vector_db"):
        """Inicializar el chatbot RAG completo"""
        self.db_path = db_path
        self.client = PersistentClient(path=db_path)
        
        # Conectar a la colecciÃ³n mejorada
        self.collection = self.client.get_collection("codehelper_csharp_improved")
        
        # Modelo de embeddings para recuperaciÃ³n
        self.embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
        
        # Cross-encoder para re-ranking (mejora la calidad de resultados)
        self.cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
        
        # Modelo LLM para generaciÃ³n (usando un modelo mÃ¡s pequeÃ±o pero efectivo)
        self.setup_llm()
        
        # Traductor para respuestas
        self.translator = pipeline("translation_en_to_es", model="Helsinki-NLP/opus-mt-en-es")
        
        # Templates de prompts mejorados
        self.setup_prompts()
        
    def setup_llm(self):
        """Configurar modelo LLM para generaciÃ³n"""
        try:
            # Por ahora, usar solo modo de recuperaciÃ³n para evitar problemas
            print("Usando modo de solo recuperaciÃ³n para mayor estabilidad...")
            self.model = None
            self.tokenizer = None
            return
            
            # CÃ³digo original comentado para referencia
            # model_name = "microsoft/DialoGPT-small"
            # self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            # self.model = AutoModelForCausalLM.from_pretrained(
            #     model_name,
            #     torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            #     device_map="auto" if torch.cuda.is_available() else "cpu"
            # )
            # 
            # if self.tokenizer.pad_token is None:
            #     self.tokenizer.pad_token = self.tokenizer.eos_token
            # 
            # self.model.config.pad_token_id = self.tokenizer.pad_token_id
            
        except Exception as e:
            print(f"Error cargando modelo LLM: {e}")
            print("Usando modo de solo recuperaciÃ³n...")
            self.model = None
            self.tokenizer = None
    
    def setup_prompts(self):
        """Configurar templates de prompts mejorados para respuestas mÃ¡s limpias"""
        self.prompts = {
            'code_example': PromptTemplate(
                input_variables=["context", "question"],
                template="""Eres un experto en C# y .NET. El usuario estÃ¡ pidiendo un ejemplo de cÃ³digo. Proporciona una respuesta clara y Ãºtil.

Contexto relevante:
{context}

Pregunta del usuario: {question}

Responde de manera directa y Ãºtil. Si hay ejemplos de cÃ³digo en el contexto, presÃ©ntalos de forma clara. Si no hay ejemplos especÃ­ficos, proporciona informaciÃ³n Ãºtil sobre el tema.

Respuesta:"""
            ),
            'concept_explanation': PromptTemplate(
                input_variables=["context", "question"],
                template="""Eres un profesor experto en C# y .NET. Explica el concepto solicitado de manera clara y directa.

Contexto relevante:
{context}

Concepto a explicar: {question}

Proporciona una explicaciÃ³n clara, concisa y fÃ¡cil de entender. Incluye ejemplos prÃ¡cticos cuando sea Ãºtil.

ExplicaciÃ³n:"""
            ),
            'syntax_help': PromptTemplate(
                input_variables=["context", "question"],
                template="""Eres un asistente de programaciÃ³n especializado en C#. Ayuda con la sintaxis solicitada.

Contexto relevante:
{context}

Pregunta sobre sintaxis: {question}

Proporciona la sintaxis correcta y ejemplos de uso claros. SÃ© directo y Ãºtil.

Respuesta:"""
            ),
            'general_help': PromptTemplate(
                input_variables=["context", "question"],
                template="""Eres un asistente experto en C# y .NET. Responde la pregunta del usuario de manera Ãºtil y clara.

Contexto relevante:
{context}

Pregunta: {question}

Proporciona una respuesta directa y Ãºtil. Si hay informaciÃ³n relevante en el contexto, Ãºsala. Si no, proporciona informaciÃ³n general Ãºtil sobre C# y .NET.

Respuesta:"""
            )
        }
    
    def classify_question(self, question: str) -> str:
        """Clasificar el tipo de pregunta para usar el prompt apropiado - MEJORADO"""
        question_lower = question.lower()
        
        # Palabras clave para ejemplos de cÃ³digo (expanded)
        code_keywords = [
            'ejemplo', 'cÃ³digo', 'implementar', 'cÃ³mo hacer', 'muestra', 'muÃ©strame',
            'dame', 'dÃ©jame', 'quiero ver', 'necesito', 'ayÃºdame con', 'cÃ³mo crear',
            'programa', 'aplicaciÃ³n', 'proyecto', 'sample', 'demo', 'tutorial',
            'ver', 'mostrar', 'enseÃ±ar', 'enseÃ±ame', 'ejemplifica', 'ilustra',
            'cÃ³digo de', 'ejemplo de', 'muestra de', 'cÃ³mo se hace', 'cÃ³mo se crea',
            'cÃ³mo implementar', 'cÃ³mo usar', 'cÃ³mo trabajar con'
        ]
        
        # Palabras clave para explicaciones de conceptos (expanded)
        concept_keywords = [
            'quÃ© es', 'explicar', 'concepto', 'definir', 'significa', 'para quÃ© sirve',
            'cuÃ¡l es', 'describe', 'caracterÃ­sticas', 'ventajas', 'desventajas',
            'definiciÃ³n', 'explicaciÃ³n', 'quÃ© significa', 'quÃ© hace', 'cÃ³mo funciona',
            'en quÃ© consiste', 'quÃ© representa', 'quÃ© implica'
        ]
        
        # Palabras clave para sintaxis (expanded)
        syntax_keywords = [
            'sintaxis', 'syntax', 'formato', 'escribir', 'declarar', 'cÃ³mo declarar',
            'estructura', 'palabra clave', 'keyword', 'operador', 'cÃ³mo se escribe',
            'cÃ³mo se declara', 'cÃ³mo se define', 'cÃ³mo se estructura', 'cÃ³mo se usa',
            'cÃ³mo se implementa', 'cÃ³mo se crea', 'cÃ³mo se define'
        ]
        
        # Verificar si es una pregunta sobre ejemplos de cÃ³digo
        if any(keyword in question_lower for keyword in code_keywords):
            return 'code_example'
        
        # Verificar si es una pregunta sobre conceptos
        if any(keyword in question_lower for keyword in concept_keywords):
            return 'concept_explanation'
        
        # Verificar si es una pregunta sobre sintaxis
        if any(keyword in question_lower for keyword in syntax_keywords):
            return 'syntax_help'
        
        # Verificar patrones especÃ­ficos que indican solicitud de ejemplos
        if any(pattern in question_lower for pattern in [
            'cÃ³mo crear', 'cÃ³mo hacer', 'cÃ³mo implementar', 'cÃ³mo usar',
            'cÃ³mo trabajar', 'cÃ³mo desarrollar', 'cÃ³mo programar'
        ]):
            return 'code_example'
        
        # Verificar si la pregunta contiene palabras que sugieren ejemplos
        if any(word in question_lower for word in ['ejemplo', 'cÃ³digo', 'muestra', 'dame', 'muÃ©strame']):
            return 'code_example'
        
        # Por defecto, usar ayuda general
        return 'general_help'
    
    def retrieve_relevant_chunks(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """Recuperar chunks relevantes usando embeddings - MEJORADO"""
        # Generar embedding de la consulta
        query_embedding = self.embedding_model.encode(query).tolist()
        
        # BÃºsqueda inicial con mÃ¡s resultados
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results * 3  # Obtener mÃ¡s resultados para re-ranking
        )
        
        # Re-ranking con cross-encoder
        if len(results['documents'][0]) > 0:
            pairs = [[query, doc] for doc in results['documents'][0]]
            scores = self.cross_encoder.predict(pairs)
            
            # Combinar documentos con scores
            doc_scores = list(zip(results['documents'][0], scores, results['metadatas'][0]))
            doc_scores.sort(key=lambda x: x[1], reverse=True)
            
            # Retornar los mejores resultados con umbral mÃ¡s bajo
            top_results = []
            for doc, score, metadata in doc_scores[:n_results]:
                # Umbral mÃ¡s bajo para capturar mÃ¡s resultados Ãºtiles
                if score > 0.3:  # Reducido de 0.5 a 0.3
                    top_results.append({
                        'content': doc,
                        'score': score,
                        'metadata': metadata
                    })
            
            return top_results
        
        return []
    
    def clean_context(self, chunks: List[Dict[str, Any]]) -> str:
        """Limpiar y preparar el contexto para el prompt - MEJORADO"""
        context_parts = []
        
        for chunk in chunks:
            # Limpiar el contenido del chunk
            content = chunk['content']
            
            # Remover caracteres extraÃ±os y normalizar
            content = re.sub(r'[^\w\s\.\,\;\:\!\?\(\)\[\]\{\}\+\-\*\/\=\<\>\"\'\n\r\t]', '', content)
            content = re.sub(r'\s+', ' ', content)
            content = content.strip()
            
            # Umbral mÃ¡s bajo para incluir mÃ¡s contenido
            if chunk['score'] > 0.3 and len(content) > 20:  # Reducido de 0.5 a 0.3
                context_parts.append(content)
        
        return "\n\n".join(context_parts)
    
    def generate_response(self, context: str, question: str, question_type: str) -> str:
        """Generar respuesta usando el modelo LLM o modo de recuperaciÃ³n - MEJORADO"""
        if self.model is None:
            # Modo de solo recuperaciÃ³n - proporcionar respuestas estructuradas
            if not context:
                # Respuesta mÃ¡s Ãºtil cuando no hay contexto
                if question_type == 'code_example':
                    return """AquÃ­ tienes un ejemplo bÃ¡sico de cÃ³digo en C# y .NET:

**Ejemplo de programa bÃ¡sico en C#:**
```csharp
using System;

namespace MiPrimerPrograma
{
    class Program
    {
        static void Main(string[] args)
        {
            Console.WriteLine("Â¡Hola Mundo desde C#!");
            
            // Variables y tipos de datos
            string nombre = "Desarrollador";
            int edad = 25;
            double salario = 50000.50;
            
            Console.WriteLine($"Nombre: {nombre}");
            Console.WriteLine($"Edad: {edad}");
            Console.WriteLine($"Salario: ${salario:F2}");
            
            // Estructuras de control
            if (edad >= 18)
            {
                Console.WriteLine("Eres mayor de edad");
            }
            
            // Bucle for
            for (int i = 1; i <= 5; i++)
            {
                Console.WriteLine($"IteraciÃ³n {i}");
            }
        }
    }
}
```

**Ejemplo de clase en C#:**
```csharp
public class Persona
{
    public string Nombre { get; set; }
    public int Edad { get; set; }
    
    public Persona(string nombre, int edad)
    {
        Nombre = nombre;
        Edad = edad;
    }
    
    public void Presentarse()
    {
        Console.WriteLine($"Hola, soy {Nombre} y tengo {Edad} aÃ±os");
    }
}
```

Â¿Te gustarÃ­a ver ejemplos mÃ¡s especÃ­ficos de algÃºn tema en particular?"""
                elif question_type == 'concept_explanation':
                    return """AquÃ­ tienes informaciÃ³n general sobre C# y .NET:

**C#** es un lenguaje de programaciÃ³n moderno, orientado a objetos y de propÃ³sito general desarrollado por Microsoft como parte de la plataforma .NET.

**CaracterÃ­sticas principales de C#:**
- Tipado estÃ¡tico y seguro
- Orientado a objetos
- Garbage collection automÃ¡tico
- LINQ para consultas de datos
- Soporte para programaciÃ³n asÃ­ncrona
- Multiplataforma

**.NET** es una plataforma de desarrollo que incluye:
- Common Language Runtime (CLR)
- Framework Class Library (FCL)
- Herramientas de desarrollo
- Soporte para mÃºltiples lenguajes

Â¿Te gustarÃ­a que profundice en algÃºn aspecto especÃ­fico?"""
                elif question_type == 'syntax_help':
                    return """AquÃ­ tienes informaciÃ³n sobre la sintaxis bÃ¡sica de C#:

**DeclaraciÃ³n de variables:**
```csharp
int numero = 10;
string texto = "Hola";
bool activo = true;
double precio = 19.99;
```

**DeclaraciÃ³n de mÃ©todos:**
```csharp
public void MetodoVacio() { }
public int MetodoConRetorno() { return 42; }
public string MetodoConParametros(string nombre) { return $"Hola {nombre}"; }
```

**DeclaraciÃ³n de clases:**
```csharp
public class MiClase
{
    public string Propiedad { get; set; }
    
    public MiClase() { }
    
    public void MiMetodo() { }
}
```

Â¿Necesitas ayuda con alguna sintaxis especÃ­fica?"""
                else:
                    return "No encontrÃ© informaciÃ³n especÃ­fica para tu pregunta. Â¿PodrÃ­as reformularla o ser mÃ¡s especÃ­fico?"
            
            # Crear respuesta estructurada basada en el contexto
            if question_type == 'code_example':
                # Para ejemplos de cÃ³digo - respuesta mÃ¡s especÃ­fica
                return f"AquÃ­ tienes informaciÃ³n relevante con ejemplos de cÃ³digo:\n\n{context[:800]}..."
            elif question_type == 'concept_explanation':
                # Para explicaciones de conceptos
                return f"BasÃ¡ndome en la informaciÃ³n disponible:\n\n{context[:600]}..."
            elif question_type == 'syntax_help':
                # Para ayuda de sintaxis
                return f"InformaciÃ³n sobre sintaxis:\n\n{context[:600]}..."
            else:
                # Respuesta general
                return f"InformaciÃ³n relevante:\n\n{context[:600]}..."
        
        try:
            # Seleccionar prompt apropiado
            prompt_template = self.prompts[question_type]
            prompt = prompt_template.format(context=context, question=question)
            
            # Tokenizar con attention mask explÃ­cito
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=1024,
                padding=True,
                return_attention_mask=True
            )
            
            # Generar respuesta
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_length=inputs['input_ids'].shape[1] + 100,  # Respuestas mÃ¡s cortas
                    temperature=0.7,  # Balance entre creatividad y precisiÃ³n
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,  # Evitar repeticiones
                    no_repeat_ngram_size=3   # Evitar repeticiÃ³n de frases
                )
            
            # Decodificar respuesta
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extraer solo la parte generada (despuÃ©s del prompt)
            response = response[len(prompt):].strip()
            
            # Limpiar respuesta
            response = re.sub(r'^Respuesta:\s*', '', response)
            response = re.sub(r'^ExplicaciÃ³n:\s*', '', response)
            response = re.sub(r'\n+', '\n', response)  # Normalizar saltos de lÃ­nea
            
            # Si la respuesta estÃ¡ vacÃ­a o es muy corta, usar fallback
            if not response or len(response) < 20:
                if context:
                    return f"BasÃ¡ndome en la informaciÃ³n disponible:\n\n{context[:300]}..."
                else:
                    return "No pude generar una respuesta especÃ­fica. Â¿PodrÃ­as reformular tu pregunta?"
            
            return response
            
        except Exception as e:
            print(f"Error en generaciÃ³n: {e}")
            if context:
                return f"BasÃ¡ndome en la informaciÃ³n disponible:\n\n{context[:300]}..."
            else:
                return "Error generando respuesta. Â¿PodrÃ­as reformular tu pregunta?"
    
    def translate_response(self, response: str) -> str:
        """Traducir respuesta al espaÃ±ol si es necesario"""
        try:
            # Detectar si ya estÃ¡ en espaÃ±ol
            spanish_words = ['es', 'son', 'estÃ¡', 'estÃ¡n', 'para', 'con', 'por', 'que', 'como', 'cuando', 'una', 'las', 'los']
            spanish_count = sum(1 for word in spanish_words if word in response.lower())
            
            if spanish_count >= 2:  # Si tiene al menos 2 palabras en espaÃ±ol
                return response
            
            # Traducir solo si es necesario
            translated = self.translator(response[:400])[0]["translation_text"]
            return translated
        except:
            return response
    
    def chat(self, question: str) -> str:
        """Proceso completo de chat RAG - versiÃ³n MEJORADA"""
        # 1. Clasificar pregunta
        question_type = self.classify_question(question)
        
        # 2. Recuperar contexto relevante
        relevant_chunks = self.retrieve_relevant_chunks(question, n_results=3)
        
        # 3. Preparar contexto limpio
        context = self.clean_context(relevant_chunks)
        
        # 4. Generar respuesta
        response = self.generate_response(context, question, question_type)
        
        # 5. Traducir si es necesario
        response = self.translate_response(response)
        
        return response
    
    def interactive_chat(self):
        """Modo interactivo de chat - versiÃ³n limpia"""
        print("ğŸ¤– ChatBot RAG para C# y .NET")
        print("Escribe 'salir' para terminar")
        print("-" * 40)
        
        while True:
            try:
                question = input("\nğŸ‘¤ TÃº: ").strip()
                
                if question.lower() in ['salir', 'exit', 'quit']:
                    print("ğŸ‘‹ Â¡Hasta luego!")
                    break
                
                if not question:
                    continue
                
                # Procesar pregunta y mostrar respuesta limpia
                response = self.chat(question)
                print(f"\nğŸ¤– {response}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Â¡Hasta luego!")
                break
            except Exception as e:
                print(f"âŒ Error: {e}")

if __name__ == "__main__":
    chatbot = RAGChatbot()
    chatbot.interactive_chat() 